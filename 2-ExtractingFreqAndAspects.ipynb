{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,re, pickle, os, json\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from urllib.parse import quote\n",
    "import multiprocessing as mp\n",
    "\n",
    "def get_all_ngrams(text,ngram_up_to):\n",
    "    \n",
    "    global all_mentions\n",
    "\n",
    "    # removing html tags\n",
    "    content = re.sub('<[^>]+>', '', text)\n",
    "    \n",
    "    #word tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    all_ngrams = [\" \".join(x) for n in range(1,ngram_up_to+1) for x in nltk.ngrams(tokens,n)]        \n",
    "     \n",
    "    all_ngrams = [x for x in all_ngrams if x in all_mentions]\n",
    "    \n",
    "    return all_ngrams\n",
    "\n",
    "def clean_page(page):\n",
    "    \n",
    "    global ngram_up_to\n",
    "    \n",
    "    entities = [x for x in page.findAll(\"a\") if x.has_attr(\"href\")]\n",
    "     \n",
    "    page_text = page.text\n",
    "    all_ngrams = get_all_ngrams(page_text,ngram_up_to)\n",
    "\n",
    "    content_ngrams= Counter(all_ngrams)\n",
    "    \n",
    "    box_mentions = Counter([x.text for x in entities])\n",
    "    box_entities = Counter([x[\"href\"] for x in entities])\n",
    "        \n",
    "    mentions_dict = {x:[] for x in box_mentions}\n",
    "    for e in entities:\n",
    "        mentions_dict[e.text].append(e[\"href\"])\n",
    "    \n",
    "    mentions_dict = {x:Counter(y) for x,y in mentions_dict.items()} \n",
    "    \n",
    "    return [box_mentions,content_ngrams,box_entities,mentions_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections (page):\n",
    "    page = page.text.strip().split(\"\\n\")\n",
    "    sections = {\"Main\":[]}\n",
    "    dict_key = \"Main\"\n",
    "    \n",
    "    for line in page:\n",
    "        if not \"Section::::\" in line:\n",
    "            sections[dict_key].append(line)\n",
    "        else:\n",
    "            dict_key = line.replace(\"Section::::\",\"\")[:-1]\n",
    "            sections[dict_key] = []\n",
    "            \n",
    "    sections = {x:y for x,y in sections.items() if len(y)>0}\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "    content = open(proessed_docs+folder+\"/\"+doc).read()\n",
    "    content = BeautifulSoup(content).findAll(\"doc\")\n",
    "    pages = []\n",
    "    for page in content:\n",
    "        title = quote(page[\"title\"]).replace(\"/\",\"%\")\n",
    "        sections = {\"title\":title,\"sections\": get_sections(page)}\n",
    "        r = [title]+ clean_page(page) + [sections]\n",
    "        pages.append([r])\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the set of all possible mentions \n",
    "\n",
    "with open(\"all_mentions.pickle\", \"rb\") as f:\n",
    "    all_mentions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output already used before, coming from WikiExtractor\n",
    "\n",
    "proessed_docs = \"output/\"\n",
    "\n",
    "ngram_up_to = 3\n",
    "\n",
    "# again, the number of cpu\n",
    "N= mp.cpu_count()-2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    step = 1\n",
    "\n",
    "    for folder in os.listdir(proessed_docs):\n",
    "        \n",
    "            with mp.Pool(processes = N) as p:\n",
    "                res = p.map(process_doc, os.listdir(proessed_docs+folder))\n",
    "\n",
    "            res = [y for x in res for y in x]\n",
    "            \n",
    "            # separating frequency counts from aspects\n",
    "            freq_res = [x[0][:-1] for x in res]\n",
    "            sections = [x[0][-1] for x in res]\n",
    "            \n",
    "            # saving sections independently\n",
    "            for sect in sections:\n",
    "                title = sect[\"title\"]\n",
    "                s = sect[\"sections\"]\n",
    "                try:\n",
    "                    with open('Aspects/'+title+\".json\", 'w') as fp:\n",
    "                        json.dump(s, fp)\n",
    "                except OSError as e:\n",
    "                    print (e)\n",
    "                    continue\n",
    "                    \n",
    "            # storing counts, still divided in folders       \n",
    "            with open('Store-Counts/'+str(step)+\".json\", 'w') as fp:\n",
    "                json.dump(freq_res, fp)\n",
    "            \n",
    "            print(\"Done %s folders over %s\" % (step, len(os.listdir(proessed_docs))))\n",
    "            step+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
